# -*- coding: utf-8 -*-
"""Build Enviornment of Reinforcement Learning GME Trading Tutorial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NM1f6vCFFGNr3QaxGgx9UgILfhpgHzye

# 0. Install and Import dependencies

gym == 0.25.2 

stable-baselines3 => stable-baselines3
"""

!pip install tensorflow stable-baselines3 gym-anytrading gym==0.25.2

# Gym stuff
import gym
import gym_anytrading
from gym_anytrading.envs import StocksEnv

# Stable baselines - rl stuff
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3 import A2C, PPO

# Processing libraries
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

"""# 1. Load Data 

"""

df = pd.read_csv('/content/nasdaq_dataset.csv')

df.head()

df['Date'] = pd.to_datetime(df['Date'])
df.dtypes

df.set_index('Date', inplace=True)
df.drop(columns=['Time','Target'],inplace=True)
print(len(df))
df.head()

env = gym.make('stocks-v0', df=df, frame_bound=(5,10000), window_size=5)

"""Add Environment"""

def add_new_data(env):
    start = env.frame_bound[0] - env.window_size
    end = env.frame_bound[1]
    prices = env.df.loc[:, 'Close'].to_numpy()[start:end]
    signal_features = env.df.loc[:, ['Open', 'High', 'Low', 'Volume', 'NumberOfTrades', 'BidVolume',
       'AskVolume', 'ATR', 'RSI', 'AD', 'AROON_DOWN', 'AROON_UP', 'ADX',
       'STOCH_K', 'STOCH_D', 'MACD', 'MACD_SIGNAL', 'HAMMER',
       'INVERTED_HAMMER', 'ENGULFING']].to_numpy()[start:end]
    return prices, signal_features

class MyEnv(StocksEnv):
    _process_data = add_new_data
    
env2 = MyEnv(df=df, window_size=30, frame_bound=(30,18000))

env2.signal_features

"""Build new environment for all data

# 3. Build Environment and Train

MlpLstmPolicy => MlpPolicy
"""

env_maker = lambda: env2
env = DummyVecEnv([env_maker])

# model = A2C('MlpPolicy', env, verbose=1) 
# model.learn(total_timesteps=1000000)

model = PPO('MlpPolicy', env, verbose=1)
model.learn(total_timesteps=1000000)

"""# 4. Evaluation"""

env = MyEnv(df=df, window_size=30, frame_bound=(18000,18500))
obs = env.reset()
while True: 
    obs = obs[np.newaxis, ...]
    action, _states = model.predict(obs)
    obs, rewards, done, info = env.step(action)
    if done:
        print("info", info)
        break

plt.figure(figsize=(15,6))
plt.cla()
env.render_all()
plt.show()

# Specify the file path and name for saving the model
save_path = "PPO_nasdaq_model_1"

# Save the model
model.save(save_path)





from stable_baselines3 import PPO
import gym

# Define and initialize the environment
env = gym.make('stocks-v0', df=df, frame_bound=(90,110), window_size=5)
obs = env.reset()

# Define and initialize the model
model = PPO('MlpPolicy', env)

# Load the pre-trained model weights
model.load("my_model")

# Use the loaded model to interact with the environment
obs = env.reset()
done = False

while not done:
    action, _ = model.predict(obs)
    obs, reward, done, _ = env.step(action)
    env.render()

env.close()







import gym
import yfinance as yf
from stable_baselines3 import PPO

# Function to retrieve stock data using yfinance
def get_stock_data(symbol, start_date, end_date):
    stock = yf.download(symbol, start=start_date, end=end_date)
    return stock

# Define the stock symbols and training parameters
stock_symbols = ["AAPL", "TSLA"]
start_date = "2022-01-01"
end_date = "2022-12-31"
num_timesteps = 100000

# Define and initialize the reinforcement learning model
model = PPO('MlpPolicy', env)

# Train the model for each stock symbol
for symbol in stock_symbols:
    # Retrieve stock data for the symbol
    stock_data = get_stock_data(symbol, start_date, end_date)

    # Define your reinforcement learning environment
    env = gym.make('stocks-v0', df=stock_data)

    # Train the model on the stock's environment
    model.learn(total_timesteps=num_timesteps)

# Save the trained model
model.save("save_model")

env = gym.make('stocks-v0', df=df, frame_bound=(90,110), window_size=5)
obs = env.reset()
while True: 
    obs = obs[np.newaxis, ...]
    action, _states = model.predict(obs)
    obs, rewards, done, info = env.step(action)
    if done:
        print("info", info)
        break

